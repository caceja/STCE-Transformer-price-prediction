{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. STCE: Transformer + CNN\n",
        "- Architecture: Dense→n×(Multi‑Head Attention + residual) → n×(Conv1D + residual) → Dense output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEdKONyjjvYq",
        "outputId": "9a87d522-8e51-469b-c956-d6e08ff51c71"
      },
      "outputs": [],
      "source": [
        "# ─── Imports & Seeds ───────────────────────────────────────────────────────────\n",
        "import os, glob, random, time\n",
        "import numpy as np, pandas as pd\n",
        "import tensorflow as tf\n",
        "from math import sqrt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense, Dropout, LayerNormalization,\n",
        "    MultiHeadAttention, Conv1D, Add\n",
        ")\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# ─── PARAMETERS ────────────────────────────────────────────────────────────────\n",
        "INPUT_DIR   = \"data/raw\"\n",
        "OUTPUT_DIR  = \"outputs\"\n",
        "\n",
        "# ensure directories exist\n",
        "os.makedirs(INPUT_DIR,  exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# window & split sizes\n",
        "WINDOW_SIZE = 7\n",
        "SEQ_LENGTH  = WINDOW_SIZE - 1\n",
        "\n",
        "# train‑set size per crop\n",
        "TRAIN_SIZES = {\n",
        "    \"Cucumber\": 2826,\n",
        "    \"LongBean\": 2826,\n",
        "    \"Tomato\":   2826\n",
        "}\n",
        "\n",
        "# model hyperparameters\n",
        "D_MODEL       = 64\n",
        "N_HEADS       = 4\n",
        "CONV_FILTERS  = 64\n",
        "CONV_KERNEL   = 4\n",
        "DROPOUT_RATE  = 0.2\n",
        "N_LAYERS      = 2\n",
        "\n",
        "# training\n",
        "EPOCHS     = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "# ─── MODEL DEFINITION ──────────────────────────────────────────────────────────\n",
        "def transformer_model_with_cnn(\n",
        "    seq_len, num_feats, d_model, n_heads,\n",
        "    conv_filters, conv_kernel, dropout_rate, n_layers\n",
        "):\n",
        "    inp = Input(shape=(seq_len, num_feats))\n",
        "    x = inp\n",
        "\n",
        "    # attention + residual\n",
        "    for _ in range(n_layers):\n",
        "        attn = MultiHeadAttention(num_heads=n_heads, key_dim=d_model)(x, x)\n",
        "        attn = Dropout(dropout_rate)(attn)\n",
        "        attn = LayerNormalization(epsilon=1e-6)(attn)\n",
        "        x = Add()([x, attn])\n",
        "\n",
        "    # convolution + residual\n",
        "    for _ in range(n_layers):\n",
        "        c = Conv1D(filters=conv_filters, kernel_size=conv_kernel,\n",
        "                   activation=\"relu\", padding=\"same\")(x)\n",
        "        c = Dropout(dropout_rate)(c)\n",
        "        c = LayerNormalization(epsilon=1e-6)(c)\n",
        "        x = Add()([x, c])\n",
        "\n",
        "    out = Dense(units=num_feats)(x)\n",
        "    return Model(inputs=inp, outputs=out)\n",
        "\n",
        "\n",
        "# ─── SHEET PROCESSOR ───────────────────────────────────────────────────────────\n",
        "def process_sheet(df, train_size):\n",
        "    # parse & index by date\n",
        "    df[\"Tarikh\"] = pd.to_datetime(df[\"Tarikh\"]).dt.strftime(\"%d/%m/%Y\")\n",
        "    df.index = df[\"Tarikh\"]\n",
        "    df = df.drop(\"Tarikh\", axis=1).apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "    # target series\n",
        "    series = df[\"WholesalePriceNew\"].values.reshape(-1, 1)\n",
        "    train, test = series[:train_size], series[train_size:]\n",
        "\n",
        "    # scale 0–1\n",
        "    scaler = MinMaxScaler((0, 1))\n",
        "    train_s = scaler.fit_transform(train)\n",
        "    test_s  = scaler.transform(test)\n",
        "\n",
        "    # sliding windows\n",
        "    def slide(arr, w): return np.array([arr[i:i+w] for i in range(len(arr)-w+1)])\n",
        "    w = WINDOW_SIZE\n",
        "    tx = slide(train_s, w); ty = tx[:, -1]; tx = tx[:, :-1]\n",
        "    vx = slide(test_s, w);  vy = vx[:, -1]; vx = vx[:, :-1]\n",
        "\n",
        "    # build & train\n",
        "    model = transformer_model_with_cnn(\n",
        "        seq_len=w-1,\n",
        "        num_feats=1,\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        conv_filters=CONV_FILTERS,\n",
        "        conv_kernel=CONV_KERNEL,\n",
        "        dropout_rate=DROPOUT_RATE,\n",
        "        n_layers=N_LAYERS\n",
        "    )\n",
        "    model.compile(loss=MeanSquaredError(), optimizer=Adam(1e-3))\n",
        "    model.fit(tx, ty, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2)\n",
        "\n",
        "    # predict & invert scale\n",
        "    preds = model.predict(vx)[:, -1, :]\n",
        "    preds_r = scaler.inverse_transform(preds)\n",
        "    vy_r    = scaler.inverse_transform(vy)\n",
        "\n",
        "    rmse = sqrt(mean_squared_error(vy_r, preds_r))\n",
        "    mae  = mean_absolute_error(vy_r, preds_r)\n",
        "    return vy_r, preds_r, rmse, mae\n",
        "\n",
        "\n",
        "# ─── MAIN LOOP ─────────────────────────────────────────────────────────────────\n",
        "for fp in glob.glob(os.path.join(INPUT_DIR, \"*_FillKNN.xlsx\")):\n",
        "    crop = os.path.basename(fp).split(\"_\")[0]\n",
        "    ts   = TRAIN_SIZES.get(crop)\n",
        "    out_file = f\"{crop}_STCE_results.xlsx\"\n",
        "\n",
        "    # load & prepare writer\n",
        "    xls    = pd.ExcelFile(fp)\n",
        "    writer = pd.ExcelWriter(os.path.join(OUTPUT_DIR, out_file),\n",
        "                            engine=\"xlsxwriter\")\n",
        "\n",
        "    print(f\"\\n➡️ Processing {crop}…\")\n",
        "    for sheet in xls.sheet_names:\n",
        "        df = pd.read_excel(fp, sheet_name=sheet)\n",
        "        actual, pred, rmse, mae = process_sheet(df, ts)\n",
        "\n",
        "        # save sheet\n",
        "        out_df = pd.DataFrame({\n",
        "            \"Actual\":    actual.flatten(),\n",
        "            \"Predicted\": pred.flatten(),\n",
        "            \"RMSE\":      rmse,\n",
        "            \"MAE\":       mae\n",
        "        })\n",
        "        out_df.to_excel(writer, sheet_name=sheet, index=False)\n",
        "        print(f\"   • {sheet}: RMSE={rmse:.3f}, MAE={mae:.3f}\")\n",
        "\n",
        "    writer.close()\n",
        "    print(f\"✅ Saved: {os.path.join(OUTPUT_DIR, out_file)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. TCE: Transformer + CNN + Positional Encoding + MLP\n",
        "- Positional Encoding: sinusoidal\n",
        "\n",
        "- Architecture: Dense→PosEnc→Dropout → n×(Attention + residual) → n×(Conv1D + residual) → MLP(128→64) → Dense output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1_kfFZmc---c"
      },
      "outputs": [],
      "source": [
        "# ─── Imports & Seeds ───────────────────────────────────────────────────────────\n",
        "import os, glob, random, time\n",
        "import numpy as np, pandas as pd\n",
        "import tensorflow as tf\n",
        "from math import sqrt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import (Dense, Dropout, LayerNormalization,\n",
        "                                     MultiHeadAttention, Conv1D, Add, Layer)\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# ─── PARAMETERS ────────────────────────────────────────────────────────────────\n",
        "INPUT_DIR   = \"data/raw\"\n",
        "OUTPUT_DIR  = \"outputs\"\n",
        "\n",
        "# ensure directories exist\n",
        "os.makedirs(INPUT_DIR,  exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# train set size per crop (adjust as needed)\n",
        "TRAIN_SIZES = {\n",
        "    \"Cucumber\": 2826,\n",
        "    \"LongBean\": 2826,\n",
        "    \"Tomato\":   2826,\n",
        "    \"KubisBulat\": 2856\n",
        "}\n",
        "\n",
        "# sliding‑window\n",
        "WINDOW_SIZE = 7\n",
        "SEQ_LENGTH  = WINDOW_SIZE - 1\n",
        "\n",
        "# model hyperparameters\n",
        "D_MODEL       = 64\n",
        "N_HEADS       = 4\n",
        "CONV_FILTERS  = 64\n",
        "CONV_KERNEL   = 12\n",
        "DROPOUT_RATE  = 0.2\n",
        "N_LAYERS      = 2\n",
        "MLP_UNITS_1   = 128\n",
        "MLP_UNITS_2   = 64\n",
        "\n",
        "# training params\n",
        "EPOCHS     = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "# ─── Positional Encoding Layer ────────────────────────────────────────────────\n",
        "class PositionalEncoding(Layer):\n",
        "    def __init__(self, d_model, **kwargs):\n",
        "        super(PositionalEncoding, self).__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def get_config(self):\n",
        "        cfg = super(PositionalEncoding, self).get_config()\n",
        "        cfg.update({\"d_model\": self.d_model})\n",
        "        return cfg\n",
        "\n",
        "    def call(self, x):\n",
        "        pos = tf.range(tf.shape(x)[1], dtype=tf.float32)[:, tf.newaxis]\n",
        "        div = tf.exp(tf.range(0, self.d_model, 2, dtype=tf.float32) *\n",
        "                     (-tf.math.log(10000.0) / self.d_model))\n",
        "        angle = pos * div\n",
        "        pe = tf.concat([tf.sin(angle), tf.cos(angle)], axis=-1)[tf.newaxis, ...]\n",
        "        return x + pe\n",
        "\n",
        "\n",
        "# ─── Model Definition ──────────────────────────────────────────────────────────\n",
        "def build_tce_model(seq_len, num_feats):\n",
        "    inp = Input(shape=(seq_len, num_feats))\n",
        "    # project to d_model dims\n",
        "    x = Dense(D_MODEL)(inp)\n",
        "    # add positional encoding\n",
        "    x = PositionalEncoding(D_MODEL)(x)\n",
        "    x = Dropout(DROPOUT_RATE)(x)\n",
        "\n",
        "    # self‑attention blocks\n",
        "    for _ in range(N_LAYERS):\n",
        "        attn = MultiHeadAttention(num_heads=N_HEADS, key_dim=D_MODEL)(x, x)\n",
        "        attn = Dropout(DROPOUT_RATE)(attn)\n",
        "        attn = LayerNormalization(epsilon=1e-6)(attn)\n",
        "        x = Add()([x, attn])\n",
        "\n",
        "    # convolutional residual blocks\n",
        "    for _ in range(N_LAYERS):\n",
        "        c = Conv1D(filters=CONV_FILTERS, kernel_size=CONV_KERNEL,\n",
        "                   activation=\"relu\", padding=\"same\")(x)\n",
        "        c = Dropout(DROPOUT_RATE)(c)\n",
        "        c = LayerNormalization(epsilon=1e-6)(c)\n",
        "        # ensure dims match\n",
        "        if c.shape[-1] != x.shape[-1]:\n",
        "            x = Dense(c.shape[-1])(x)\n",
        "        x = Add()([x, c])\n",
        "\n",
        "    # MLP head\n",
        "    x = Dense(MLP_UNITS_1, activation=\"relu\")(x)\n",
        "    x = Dropout(DROPOUT_RATE)(x)\n",
        "    x = Dense(MLP_UNITS_2, activation=\"relu\")(x)\n",
        "    x = Dropout(DROPOUT_RATE)(x)\n",
        "\n",
        "    out = Dense(num_feats)(x)\n",
        "    return Model(inputs=inp, outputs=out)\n",
        "\n",
        "\n",
        "# ─── Sheet Processor ───────────────────────────────────────────────────────────\n",
        "def process_sheet(df, train_size):\n",
        "    # format and index\n",
        "    df[\"Tarikh\"] = pd.to_datetime(df[\"Tarikh\"]).dt.strftime(\"%d/%m/%Y\")\n",
        "    df.index = df[\"Tarikh\"]\n",
        "    df = df.drop(\"Tarikh\", axis=1).apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "    series = df[\"WholesalePriceNew\"].values.reshape(-1, 1)\n",
        "    train, test = series[:train_size], series[train_size:]\n",
        "\n",
        "    # scale 0–1\n",
        "    scaler = MinMaxScaler((0, 1))\n",
        "    train_s = scaler.fit_transform(train)\n",
        "    test_s  = scaler.transform(test)\n",
        "\n",
        "    # sliding-window\n",
        "    def slide(arr, w): return np.array([arr[i:i+w] for i in range(len(arr)-w+1)])\n",
        "    w = WINDOW_SIZE\n",
        "    tx = slide(train_s, w); ty = tx[:, -1]; tx = tx[:, :-1]\n",
        "    vx = slide(test_s, w);  vy = vx[:, -1]; vx = vx[:, :-1]\n",
        "\n",
        "    # build & train\n",
        "    model = build_tce_model(seq_len=w-1, num_feats=1)\n",
        "    model.compile(loss=MeanSquaredError(), optimizer=Adam(1e-3))\n",
        "    model.fit(tx, ty, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2)\n",
        "\n",
        "    # predict & invert scaling\n",
        "    preds = model.predict(vx)[:, -1, :]\n",
        "    preds_r = scaler.inverse_transform(preds)\n",
        "    vy_r    = scaler.inverse_transform(vy)\n",
        "\n",
        "    # metrics\n",
        "    rmse = sqrt(mean_squared_error(vy_r, preds_r))\n",
        "    mae  = mean_absolute_error(vy_r, preds_r)\n",
        "    mape = mean_absolute_percentage_error(vy_r, preds_r) * 100\n",
        "    return vy_r, preds_r, rmse, mae, mape\n",
        "\n",
        "\n",
        "# ─── Main Loop ─────────────────────────────────────────────────────────────────\n",
        "for fp in glob.glob(os.path.join(INPUT_DIR, \"*_FillKNN.xlsx\")):\n",
        "    crop      = os.path.basename(fp).split(\"_\")[0]\n",
        "    train_sz  = TRAIN_SIZES.get(crop, WINDOW_SIZE*10)  # fallback if missing\n",
        "    out_fname = f\"{crop}_TCE_results.xlsx\"\n",
        "\n",
        "    xls    = pd.ExcelFile(fp)\n",
        "    writer = pd.ExcelWriter(os.path.join(OUTPUT_DIR, out_fname),\n",
        "                            engine=\"xlsxwriter\")\n",
        "\n",
        "    print(f\"\\n➡️ Running TCE on {crop}…\")\n",
        "    for sheet in xls.sheet_names:\n",
        "        df = pd.read_excel(fp, sheet_name=sheet)\n",
        "        actual, pred, rmse, mae, mape = process_sheet(df, train_sz)\n",
        "\n",
        "        results = pd.DataFrame({\n",
        "            \"Actual\":    actual.flatten(),\n",
        "            \"Predicted\": pred.flatten(),\n",
        "            \"RMSE\":      rmse,\n",
        "            \"MAE\":       mae,\n",
        "            \"MAPE\":      mape\n",
        "        })\n",
        "        results.to_excel(writer, sheet_name=sheet, index=False)\n",
        "        print(f\"   • {sheet}: RMSE={rmse:.3f}, MAE={mae:.3f}, MAPE={mape:.2f}%\")\n",
        "\n",
        "    writer.close()\n",
        "    print(f\"✅ Saved → {os.path.join(OUTPUT_DIR, out_fname)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. STLE: Transformer + LSTM\n",
        "- Architecture: n×(Multi‑Head Attention + residual) → n×(LSTM + residual) → Dense output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT59DwnV32Wj"
      },
      "outputs": [],
      "source": [
        "# ─── Imports & Seeds ───────────────────────────────────────────────────────────\n",
        "import os, glob, random, time\n",
        "import numpy as np, pandas as pd\n",
        "import tensorflow as tf\n",
        "from math import sqrt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense, Dropout, LayerNormalization,\n",
        "    MultiHeadAttention, LSTM, Add\n",
        ")\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# ─── PARAMETERS ────────────────────────────────────────────────────────────────\n",
        "INPUT_DIR   = \"data/raw\"\n",
        "OUTPUT_DIR  = \"outputs\"\n",
        "\n",
        "# ensure directories exist\n",
        "os.makedirs(INPUT_DIR,  exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# train‑set size per crop\n",
        "TRAIN_SIZES = {\n",
        "    \"Cucumber\":   2826,\n",
        "    \"LongBean\":   2826,\n",
        "    \"KubisBulat\": 2856,\n",
        "    \"Tomato\":     2886\n",
        "}\n",
        "\n",
        "# sliding‑window\n",
        "WINDOW_SIZE = 7\n",
        "SEQ_LENGTH  = WINDOW_SIZE - 1\n",
        "\n",
        "# model hyperparameters\n",
        "D_MODEL      = 64\n",
        "N_HEADS      = 4\n",
        "LSTM_UNITS   = 64\n",
        "DROPOUT_RATE = 0.2\n",
        "N_LAYERS     = 2\n",
        "\n",
        "# training\n",
        "EPOCHS     = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "# ─── MODEL DEFINITION ──────────────────────────────────────────────────────────\n",
        "def transformer_model_with_lstm(seq_len, num_feats, d_model, n_heads, lstm_units, dropout_rate, n_layers):\n",
        "    inp = Input(shape=(seq_len, num_feats))\n",
        "    x   = inp\n",
        "\n",
        "    # self‑attention + residual\n",
        "    for _ in range(n_layers):\n",
        "        attn = MultiHeadAttention(num_heads=n_heads, key_dim=d_model)(x, x)\n",
        "        attn = Dropout(dropout_rate)(attn)\n",
        "        attn = LayerNormalization(epsilon=1e-6)(attn)\n",
        "        x    = Add()([x, attn])\n",
        "\n",
        "    # LSTM + residual\n",
        "    for _ in range(n_layers):\n",
        "        lstm = LSTM(units=lstm_units, return_sequences=True)(x)\n",
        "        lstm = Dropout(dropout_rate)(lstm)\n",
        "        lstm = LayerNormalization(epsilon=1e-6)(lstm)\n",
        "        x    = Add()([x, lstm])\n",
        "\n",
        "    out = Dense(units=num_feats)(x)\n",
        "    return Model(inputs=inp, outputs=out)\n",
        "\n",
        "\n",
        "# ─── SHEET PROCESSOR ───────────────────────────────────────────────────────────\n",
        "def process_sheet(df, train_size):\n",
        "    # format & index\n",
        "    df[\"Tarikh\"] = pd.to_datetime(df[\"Tarikh\"]).dt.strftime(\"%d/%m/%Y\")\n",
        "    df.index     = df[\"Tarikh\"]\n",
        "    df = df.drop(\"Tarikh\", axis=1).apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "    # target series\n",
        "    series = df[\"WholesalePriceNew\"].values.reshape(-1, 1)\n",
        "    train, test = series[:train_size], series[train_size:]\n",
        "\n",
        "    # scale to [0,1]\n",
        "    scaler = MinMaxScaler((0, 1))\n",
        "    train_s = scaler.fit_transform(train)\n",
        "    test_s  = scaler.transform(test)\n",
        "\n",
        "    # sliding windows\n",
        "    def slide(arr, w):\n",
        "        return np.array([arr[i : i + w] for i in range(len(arr) - w + 1)])\n",
        "    w  = WINDOW_SIZE\n",
        "    tx = slide(train_s, w); ty = tx[:, -1]; tx = tx[:, :-1]\n",
        "    vx = slide(test_s,  w); vy = vx[:, -1]; vx = vx[:, :-1]\n",
        "\n",
        "    # build & train\n",
        "    model = transformer_model_with_lstm(\n",
        "        seq_len      = w - 1,\n",
        "        num_feats    = 1,\n",
        "        d_model      = D_MODEL,\n",
        "        n_heads      = N_HEADS,\n",
        "        lstm_units   = LSTM_UNITS,\n",
        "        dropout_rate = DROPOUT_RATE,\n",
        "        n_layers     = N_LAYERS\n",
        "    )\n",
        "    model.compile(loss=MeanSquaredError(), optimizer=Adam(1e-3))\n",
        "    model.fit(tx, ty, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2)\n",
        "\n",
        "    # predict & invert scale\n",
        "    preds   = model.predict(vx)[:, -1, :]\n",
        "    preds_r = scaler.inverse_transform(preds)\n",
        "    vy_r    = scaler.inverse_transform(vy)\n",
        "\n",
        "    # compute metrics\n",
        "    rmse = sqrt(mean_squared_error(vy_r, preds_r))\n",
        "    mae  = mean_absolute_error(vy_r, preds_r)\n",
        "    return vy_r, preds_r, rmse, mae\n",
        "\n",
        "\n",
        "# ─── MAIN LOOP ─────────────────────────────────────────────────────────────────\n",
        "for fp in glob.glob(os.path.join(INPUT_DIR, \"*_FillKNN.xlsx\")):\n",
        "    crop     = os.path.basename(fp).split(\"_\")[0]\n",
        "    ts       = TRAIN_SIZES.get(crop, WINDOW_SIZE*10)\n",
        "    out_file = f\"{crop}_STLE_results.xlsx\"\n",
        "\n",
        "    xls    = pd.ExcelFile(fp)\n",
        "    writer = pd.ExcelWriter(os.path.join(OUTPUT_DIR, out_file), engine=\"xlsxwriter\")\n",
        "\n",
        "    print(f\"\\n➡️ Processing {crop} with STLE…\")\n",
        "    for sheet in xls.sheet_names:\n",
        "        df     = pd.read_excel(fp, sheet_name=sheet)\n",
        "        actual, pred, rmse, mae = process_sheet(df, ts)\n",
        "\n",
        "        results = pd.DataFrame({\n",
        "            \"Actual\":    actual.flatten(),\n",
        "            \"Predicted\": pred.flatten(),\n",
        "            \"RMSE\":      rmse,\n",
        "            \"MAE\":       mae\n",
        "        })\n",
        "        results.to_excel(writer, sheet_name=sheet, index=False)\n",
        "        print(f\"   • {sheet}: RMSE={rmse:.3f}, MAE={mae:.3f}\")\n",
        "\n",
        "    writer.close()\n",
        "    print(f\"✅ Saved → {os.path.join(OUTPUT_DIR, out_file)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. TFE: Transformer + FFN\n",
        "- Flow: n×(Attention + residual) → n×(Position‑wise Feed‑Forward + residual) → Dense\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVO6EJh3jkF6"
      },
      "outputs": [],
      "source": [
        "# ─── Imports & Seeds ───────────────────────────────────────────────────────────\n",
        "import os, glob, random, time\n",
        "import numpy as np, pandas as pd\n",
        "import tensorflow as tf\n",
        "from math import sqrt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, MultiHeadAttention, Add\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# ─── PARAMETERS ────────────────────────────────────────────────────────────────\n",
        "INPUT_DIR   = \"data/raw\"\n",
        "OUTPUT_DIR  = \"outputs\"\n",
        "\n",
        "# ensure directories exist\n",
        "os.makedirs(INPUT_DIR,  exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# train‑set size per crop\n",
        "TRAIN_SIZES = {\n",
        "    \"Cucumber\":   2826,\n",
        "    \"LongBean\":   2826,\n",
        "    \"KubisBulat\": 2856,\n",
        "    \"Tomato\":     2886\n",
        "}\n",
        "\n",
        "# sliding‑window\n",
        "WINDOW_SIZE = 7\n",
        "SEQ_LENGTH  = WINDOW_SIZE - 1\n",
        "\n",
        "# model hyperparameters\n",
        "D_MODEL      = 64\n",
        "N_HEADS      = 4\n",
        "DENSE_UNITS  = 128\n",
        "DROPOUT_RATE = 0.2\n",
        "N_LAYERS     = 2\n",
        "\n",
        "# training\n",
        "EPOCHS     = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "# ─── MODEL DEFINITION ──────────────────────────────────────────────────────────\n",
        "def build_tfe_model(seq_len, num_feats):\n",
        "    inp = Input(shape=(seq_len, num_feats))\n",
        "    x   = inp\n",
        "\n",
        "    # Multi‑Head Self‑Attention + residual\n",
        "    for _ in range(N_LAYERS):\n",
        "        attn = MultiHeadAttention(num_heads=N_HEADS, key_dim=D_MODEL)(x, x)\n",
        "        attn = Dropout(DROPOUT_RATE)(attn)\n",
        "        attn = LayerNormalization(epsilon=1e-6)(attn)\n",
        "        x    = Add()([x, attn])\n",
        "\n",
        "    # Position‑wise Feed‑Forward + residual\n",
        "    for _ in range(N_LAYERS):\n",
        "        ff = Dense(units=DENSE_UNITS, activation=\"relu\")(x)\n",
        "        ff = Dropout(DROPOUT_RATE)(ff)\n",
        "        ff = LayerNormalization(epsilon=1e-6)(ff)\n",
        "        x  = Add()([x, ff])\n",
        "\n",
        "    out = Dense(units=num_feats)(x)\n",
        "    return Model(inputs=inp, outputs=out)\n",
        "\n",
        "\n",
        "# ─── SHEET PROCESSOR ───────────────────────────────────────────────────────────\n",
        "def process_sheet(df, train_size):\n",
        "    # format dates & index\n",
        "    df[\"Tarikh\"] = pd.to_datetime(df[\"Tarikh\"]).dt.strftime(\"%d/%m/%Y\")\n",
        "    df.index     = df[\"Tarikh\"]\n",
        "    df = df.drop(\"Tarikh\", axis=1).apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "    # extract series\n",
        "    series = df[\"WholesalePriceNew\"].values.reshape(-1, 1)\n",
        "    train, test = series[:train_size], series[train_size:]\n",
        "\n",
        "    # scale to [0,1]\n",
        "    scaler = MinMaxScaler((0, 1))\n",
        "    train_s = scaler.fit_transform(train)\n",
        "    test_s  = scaler.transform(test)\n",
        "\n",
        "    # sliding windows\n",
        "    def slide(arr, w): return np.array([arr[i:i+w] for i in range(len(arr)-w+1)])\n",
        "    w  = WINDOW_SIZE\n",
        "    tx = slide(train_s, w); ty = tx[:, -1]; tx = tx[:, :-1]\n",
        "    vx = slide(test_s,  w); vy = vx[:, -1]; vx = vx[:, :-1]\n",
        "\n",
        "    # build & train\n",
        "    model = build_tfe_model(seq_len=w-1, num_feats=1)\n",
        "    model.compile(loss=MeanSquaredError(), optimizer=Adam(1e-3))\n",
        "    model.fit(tx, ty, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2)\n",
        "\n",
        "    # predict & invert scaling\n",
        "    preds   = model.predict(vx)[:, -1, :]\n",
        "    preds_r = scaler.inverse_transform(preds)\n",
        "    vy_r    = scaler.inverse_transform(vy)\n",
        "\n",
        "    # metrics\n",
        "    rmse = sqrt(mean_squared_error(vy_r, preds_r))\n",
        "    mae  = mean_absolute_error(vy_r, preds_r)\n",
        "    return vy_r, preds_r, rmse, mae\n",
        "\n",
        "\n",
        "# ─── MAIN LOOP ─────────────────────────────────────────────────────────────────\n",
        "for filepath in glob.glob(os.path.join(INPUT_DIR, \"*_FillKNN.xlsx\")):\n",
        "    crop      = os.path.basename(filepath).split(\"_\")[0]\n",
        "    train_sz  = TRAIN_SIZES.get(crop, WINDOW_SIZE*10)  # fallback\n",
        "    out_fname = f\"{crop}_TFE_results.xlsx\"\n",
        "\n",
        "    xls    = pd.ExcelFile(filepath)\n",
        "    writer = pd.ExcelWriter(os.path.join(OUTPUT_DIR, out_fname),\n",
        "                            engine=\"xlsxwriter\")\n",
        "\n",
        "    print(f\"\\n➡️ Processing {crop} with TFE…\")\n",
        "    for sheet in xls.sheet_names:\n",
        "        df      = pd.read_excel(filepath, sheet_name=sheet)\n",
        "        actual, pred, rmse, mae = process_sheet(df, train_sz)\n",
        "\n",
        "        results = pd.DataFrame({\n",
        "            \"Actual\":    actual.flatten(),\n",
        "            \"Predicted\": pred.flatten(),\n",
        "            \"RMSE\":      rmse,\n",
        "            \"MAE\":       mae\n",
        "        })\n",
        "        results.to_excel(writer, sheet_name=sheet, index=False)\n",
        "        print(f\"   • {sheet}: RMSE={rmse:.3f}, MAE={mae:.3f}\")\n",
        "\n",
        "    writer.close()\n",
        "    print(f\"✅ Saved → {os.path.join(OUTPUT_DIR, out_fname)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. LSTM \n",
        "- use as the Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dip6J4YZjctD"
      },
      "outputs": [],
      "source": [
        "# ─── Imports & Seeds ───────────────────────────────────────────────────────────\n",
        "import os, glob, random, time\n",
        "import numpy as np, pandas as pd\n",
        "import tensorflow as tf\n",
        "from math import sqrt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# ─── PARAMETERS ────────────────────────────────────────────────────────────────\n",
        "INPUT_DIR    = \"data/raw\"\n",
        "OUTPUT_DIR   = \"outputs\"\n",
        "TIMESTEPS    = 7     # sliding window length\n",
        "DAY_PREDICT  = 90    # how many days at end used for testing\n",
        "EPOCHS       = 100\n",
        "BATCH_SIZE   = 32\n",
        "\n",
        "# ensure directories exist\n",
        "os.makedirs(INPUT_DIR,  exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# ─── SHEET PROCESSOR ───────────────────────────────────────────────────────────\n",
        "def process_sheet(df):\n",
        "    # format dates & index\n",
        "    df[\"Tarikh\"] = pd.to_datetime(df[\"Tarikh\"]).dt.strftime(\"%d/%m/%Y\")\n",
        "    df.index     = df[\"Tarikh\"]\n",
        "    df = df.drop(\"Tarikh\", axis=1).apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "    # extract series and reshape\n",
        "    series = df[\"WholesalePriceNew\"].values.reshape(-1, 1)\n",
        "\n",
        "    # train/test split\n",
        "    train_len = len(series) - DAY_PREDICT\n",
        "    train, test = series[:train_len], series[train_len:]\n",
        "\n",
        "    # scale to [0,1]\n",
        "    scaler = MinMaxScaler((0, 1))\n",
        "    train_s = scaler.fit_transform(train)\n",
        "    test_s  = scaler.transform(test)\n",
        "\n",
        "    # create sliding windows\n",
        "    x_train, y_train = [], []\n",
        "    for i in range(TIMESTEPS, len(train_s)):\n",
        "        x_train.append(train_s[i-TIMESTEPS:i, 0])\n",
        "        y_train.append(train_s[i, 0])\n",
        "    x_train = np.array(x_train).reshape(-1, TIMESTEPS, 1)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    # build LSTM model\n",
        "    model = Sequential([\n",
        "        LSTM(100, return_sequences=True, input_shape=(TIMESTEPS, 1)),\n",
        "        LSTM(100, return_sequences=False),\n",
        "        Dense(50, activation=\"relu\"),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(1e-3), loss=\"mse\")\n",
        "    model.fit(x_train, y_train, epochs=EPOCHS,\n",
        "              batch_size=BATCH_SIZE, verbose=2)\n",
        "\n",
        "    # prepare test windows\n",
        "    x_test = []\n",
        "    full = np.vstack((train_s[-TIMESTEPS:], test_s))\n",
        "    for i in range(TIMESTEPS, len(full)):\n",
        "        x_test.append(full[i-TIMESTEPS:i, 0])\n",
        "    x_test = np.array(x_test).reshape(-1, TIMESTEPS, 1)\n",
        "\n",
        "    # predict & invert scale\n",
        "    preds = model.predict(x_test)\n",
        "    preds_r = scaler.inverse_transform(preds)\n",
        "    y_test  = test.reshape(-1, 1)\n",
        "\n",
        "    # metrics\n",
        "    rmse = sqrt(mean_squared_error(y_test, preds_r))\n",
        "    mae  = mean_absolute_error(y_test, preds_r)\n",
        "    mape = mean_absolute_percentage_error(y_test, preds_r) * 100\n",
        "\n",
        "    return y_test.flatten(), preds_r.flatten(), rmse, mae, mape\n",
        "\n",
        "\n",
        "# ─── MAIN LOOP ─────────────────────────────────────────────────────────────────\n",
        "for fp in glob.glob(os.path.join(INPUT_DIR, \"*_FillKNN.xlsx\")):\n",
        "    crop      = os.path.basename(fp).split(\"_\")[0]\n",
        "    out_fname = f\"{crop}_LSTM_results.xlsx\"\n",
        "    writer    = pd.ExcelWriter(os.path.join(OUTPUT_DIR, out_fname),\n",
        "                               engine=\"xlsxwriter\")\n",
        "\n",
        "    print(f\"\\n➡️ Running LSTM for {crop}…\")\n",
        "    xls = pd.ExcelFile(fp)\n",
        "    for sheet in xls.sheet_names:\n",
        "        df = pd.read_excel(fp, sheet_name=sheet)\n",
        "        actual, preds, rmse, mae, mape = process_sheet(df)\n",
        "\n",
        "        # assemble results\n",
        "        out_df = pd.DataFrame({\n",
        "            \"Actual\":    actual,\n",
        "            \"Predicted\": preds,\n",
        "            \"RMSE\":      rmse,\n",
        "            \"MAE\":       mae,\n",
        "            \"MAPE\":      mape\n",
        "        })\n",
        "        out_df.to_excel(writer, sheet_name=sheet, index=False)\n",
        "        print(f\"   • {sheet}: RMSE={rmse:.3f}, MAE={mae:.3f}, MAPE={mape:.2f}%\")\n",
        "\n",
        "    writer.close()\n",
        "    print(f\"✅ Saved → {os.path.join(OUTPUT_DIR, out_fname)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. CNN\n",
        "- Used as a benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-MH5wbIjMiD"
      },
      "outputs": [],
      "source": [
        "# ─── Imports & Seeds ───────────────────────────────────────────────────────────\n",
        "import os, glob, random, time\n",
        "import numpy as np, pandas as pd\n",
        "import tensorflow as tf\n",
        "from math import sqrt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# ─── PARAMETERS ────────────────────────────────────────────────────────────────\n",
        "INPUT_DIR   = \"data/raw\"\n",
        "OUTPUT_DIR  = \"outputs\"\n",
        "\n",
        "TIMESTEPS   = 7    # sliding window length\n",
        "DAY_PREDICT = 90   # days at end used as test set\n",
        "EPOCHS      = 100\n",
        "BATCH_SIZE  = 32\n",
        "\n",
        "# ensure directories exist\n",
        "os.makedirs(INPUT_DIR,  exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# ─── SHEET PROCESSOR ───────────────────────────────────────────────────────────\n",
        "def process_sheet(df):\n",
        "    # format dates & index\n",
        "    df[\"Tarikh\"] = pd.to_datetime(df[\"Tarikh\"]).dt.strftime(\"%d/%m/%Y\")\n",
        "    df.index     = df[\"Tarikh\"]\n",
        "    df = df.drop(\"Tarikh\", axis=1).apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "    # extract series\n",
        "    series = df[\"WholesalePriceNew\"].values.reshape(-1, 1)\n",
        "\n",
        "    # train/test split\n",
        "    train_len = len(series) - DAY_PREDICT\n",
        "    train, test = series[:train_len], series[train_len:]\n",
        "\n",
        "    # scale to [0,1]\n",
        "    scaler = MinMaxScaler((0, 1))\n",
        "    train_s = scaler.fit_transform(train)\n",
        "    test_s  = scaler.transform(test)\n",
        "\n",
        "    # build training windows\n",
        "    x_train, y_train = [], []\n",
        "    for i in range(TIMESTEPS, len(train_s)):\n",
        "        x_train.append(train_s[i-TIMESTEPS:i, 0])\n",
        "        y_train.append(train_s[i, 0])\n",
        "    x_train = np.array(x_train).reshape(-1, TIMESTEPS, 1)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    # build model\n",
        "    model = Sequential([\n",
        "        Conv1D(64, 3, activation=\"relu\", input_shape=(TIMESTEPS, 1)),\n",
        "        Flatten(),\n",
        "        Dense(50, activation=\"relu\"),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(1e-3), loss=\"mse\")\n",
        "    model.fit(x_train, y_train, epochs=EPOCHS,\n",
        "              batch_size=BATCH_SIZE, verbose=2)\n",
        "\n",
        "    # build test windows (include last train window)\n",
        "    full = np.vstack((train_s[-TIMESTEPS:], test_s))\n",
        "    x_test = [ full[i-TIMESTEPS:i, 0] for i in range(TIMESTEPS, len(full)) ]\n",
        "    x_test = np.array(x_test).reshape(-1, TIMESTEPS, 1)\n",
        "\n",
        "    # predict & unscale\n",
        "    preds    = model.predict(x_test)\n",
        "    preds_r  = scaler.inverse_transform(preds)\n",
        "    y_test_r = test.reshape(-1, 1)\n",
        "\n",
        "    # metrics\n",
        "    rmse = sqrt(mean_squared_error(y_test_r, preds_r))\n",
        "    mae  = mean_absolute_error(y_test_r, preds_r)\n",
        "    mape = mean_absolute_percentage_error(y_test_r, preds_r) * 100\n",
        "\n",
        "    return y_test_r.flatten(), preds_r.flatten(), rmse, mae, mape\n",
        "\n",
        "\n",
        "# ─── MAIN LOOP ─────────────────────────────────────────────────────────────────\n",
        "for fp in glob.glob(os.path.join(INPUT_DIR, \"*_FillKNN.xlsx\")):\n",
        "    crop      = os.path.basename(fp).split(\"_\")[0]\n",
        "    out_fname = f\"{crop}_CNN_results.xlsx\"\n",
        "    writer    = pd.ExcelWriter(os.path.join(OUTPUT_DIR, out_fname),\n",
        "                               engine=\"xlsxwriter\")\n",
        "\n",
        "    print(f\"\\n➡️ Running CNN for {crop}…\")\n",
        "    xls = pd.ExcelFile(fp)\n",
        "    for sheet in xls.sheet_names:\n",
        "        df = pd.read_excel(fp, sheet_name=sheet)\n",
        "        actual, preds, rmse, mae, mape = process_sheet(df)\n",
        "\n",
        "        # assemble and save\n",
        "        out_df = pd.DataFrame({\n",
        "            \"Actual\":    actual,\n",
        "            \"Predicted\": preds,\n",
        "            \"RMSE\":      rmse,\n",
        "            \"MAE\":       mae,\n",
        "            \"MAPE\":      mape\n",
        "        })\n",
        "        out_df.to_excel(writer, sheet_name=sheet, index=False)\n",
        "        print(f\"   • {sheet}: RMSE={rmse:.3f}, MAE={mae:.3f}, MAPE={mape:.2f}%\")\n",
        "\n",
        "    writer.close()\n",
        "    print(f\"✅ Saved → {os.path.join(OUTPUT_DIR, out_fname)}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
