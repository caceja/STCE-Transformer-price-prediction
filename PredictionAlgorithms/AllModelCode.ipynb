{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. STCE: Transformer + CNN\n",
        "Architecture: Dense→n×(Multi‑Head Attention + residual) → n×(Conv1D + residual) → Dense output\n",
        "\n",
        "Window: 7 days (6→1)\n",
        "\n",
        "Metrics: RMSE, MAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEdKONyjjvYq",
        "outputId": "9a87d522-8e51-469b-c956-d6e08ff51c71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "➡️ Processing Cucumber…\n",
            "Epoch 1/100\n",
            "89/89 - 6s - 67ms/step - loss: 0.5938\n",
            "Epoch 2/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.1184\n",
            "Epoch 3/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0585\n",
            "Epoch 4/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0363\n",
            "Epoch 5/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0269\n",
            "Epoch 6/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0202\n",
            "Epoch 7/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0160\n",
            "Epoch 8/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0136\n",
            "Epoch 9/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0121\n",
            "Epoch 10/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0103\n",
            "Epoch 11/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0092\n",
            "Epoch 12/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0081\n",
            "Epoch 13/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0074\n",
            "Epoch 14/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0067\n",
            "Epoch 15/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0065\n",
            "Epoch 16/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0059\n",
            "Epoch 17/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0055\n",
            "Epoch 18/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0053\n",
            "Epoch 19/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0051\n",
            "Epoch 20/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0047\n",
            "Epoch 21/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0047\n",
            "Epoch 22/100\n",
            "89/89 - 1s - 12ms/step - loss: 0.0046\n",
            "Epoch 23/100\n",
            "89/89 - 1s - 12ms/step - loss: 0.0043\n",
            "Epoch 24/100\n",
            "89/89 - 1s - 14ms/step - loss: 0.0043\n",
            "Epoch 25/100\n",
            "89/89 - 1s - 13ms/step - loss: 0.0041\n",
            "Epoch 26/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0039\n",
            "Epoch 27/100\n",
            "89/89 - 1s - 14ms/step - loss: 0.0039\n",
            "Epoch 28/100\n",
            "89/89 - 1s - 15ms/step - loss: 0.0039\n",
            "Epoch 29/100\n",
            "89/89 - 1s - 13ms/step - loss: 0.0038\n",
            "Epoch 30/100\n",
            "89/89 - 1s - 12ms/step - loss: 0.0037\n",
            "Epoch 31/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0036\n",
            "Epoch 32/100\n",
            "89/89 - 1s - 12ms/step - loss: 0.0036\n",
            "Epoch 33/100\n",
            "89/89 - 1s - 13ms/step - loss: 0.0035\n",
            "Epoch 34/100\n",
            "89/89 - 1s - 12ms/step - loss: 0.0035\n",
            "Epoch 35/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0035\n",
            "Epoch 36/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0035\n",
            "Epoch 37/100\n",
            "89/89 - 1s - 12ms/step - loss: 0.0034\n",
            "Epoch 38/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0033\n",
            "Epoch 39/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0033\n",
            "Epoch 40/100\n",
            "89/89 - 1s - 12ms/step - loss: 0.0033\n",
            "Epoch 41/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0033\n",
            "Epoch 42/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0032\n",
            "Epoch 43/100\n",
            "89/89 - 1s - 12ms/step - loss: 0.0032\n",
            "Epoch 44/100\n",
            "89/89 - 1s - 12ms/step - loss: 0.0032\n",
            "Epoch 45/100\n",
            "89/89 - 1s - 12ms/step - loss: 0.0031\n",
            "Epoch 46/100\n",
            "89/89 - 1s - 12ms/step - loss: 0.0031\n",
            "Epoch 47/100\n",
            "89/89 - 1s - 12ms/step - loss: 0.0031\n",
            "Epoch 48/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0031\n",
            "Epoch 49/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0031\n",
            "Epoch 50/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0031\n",
            "Epoch 51/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0030\n",
            "Epoch 52/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0031\n",
            "Epoch 53/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0029\n",
            "Epoch 54/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0030\n",
            "Epoch 55/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0030\n",
            "Epoch 56/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0029\n",
            "Epoch 57/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0029\n",
            "Epoch 58/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0030\n",
            "Epoch 59/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0029\n",
            "Epoch 60/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0029\n",
            "Epoch 61/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0028\n",
            "Epoch 62/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0029\n",
            "Epoch 63/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0029\n",
            "Epoch 64/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0029\n",
            "Epoch 65/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0028\n",
            "Epoch 66/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0028\n",
            "Epoch 67/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0028\n",
            "Epoch 68/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0028\n",
            "Epoch 69/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0028\n",
            "Epoch 70/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0028\n",
            "Epoch 71/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0028\n",
            "Epoch 72/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0027\n",
            "Epoch 73/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0027\n",
            "Epoch 74/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0027\n",
            "Epoch 75/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0027\n",
            "Epoch 76/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0027\n",
            "Epoch 77/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0027\n",
            "Epoch 78/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0027\n",
            "Epoch 79/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0027\n",
            "Epoch 80/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0027\n",
            "Epoch 81/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0027\n",
            "Epoch 82/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0027\n",
            "Epoch 83/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0027\n",
            "Epoch 84/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0026\n",
            "Epoch 85/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0027\n",
            "Epoch 86/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0026\n",
            "Epoch 87/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0026\n",
            "Epoch 88/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0026\n",
            "Epoch 89/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0026\n",
            "Epoch 90/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0026\n",
            "Epoch 91/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0027\n",
            "Epoch 92/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0026\n",
            "Epoch 93/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0027\n",
            "Epoch 94/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0026\n",
            "Epoch 95/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0026\n",
            "Epoch 96/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0026\n",
            "Epoch 97/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0026\n",
            "Epoch 98/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0026\n",
            "Epoch 99/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0026\n",
            "Epoch 100/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0025\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 144ms/step\n",
            "   • JOHOR BAHRU, JOHOR: RMSE=0.075, MAE=0.037\n",
            "Epoch 1/100\n",
            "89/89 - 7s - 75ms/step - loss: 0.7379\n",
            "Epoch 2/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.1658\n",
            "Epoch 3/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0914\n",
            "Epoch 4/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0597\n",
            "Epoch 5/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0439\n",
            "Epoch 6/100\n",
            "89/89 - 1s - 12ms/step - loss: 0.0335\n",
            "Epoch 7/100\n",
            "89/89 - 1s - 12ms/step - loss: 0.0266\n",
            "Epoch 8/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0227\n",
            "Epoch 9/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0188\n",
            "Epoch 10/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0161\n",
            "Epoch 11/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0143\n",
            "Epoch 12/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0127\n",
            "Epoch 13/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0114\n",
            "Epoch 14/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0100\n",
            "Epoch 15/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0097\n",
            "Epoch 16/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0088\n",
            "Epoch 17/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0082\n",
            "Epoch 18/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0077\n",
            "Epoch 19/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0073\n",
            "Epoch 20/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0070\n",
            "Epoch 21/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0067\n",
            "Epoch 22/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0064\n",
            "Epoch 23/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0062\n",
            "Epoch 24/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0060\n",
            "Epoch 25/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0060\n",
            "Epoch 26/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0058\n",
            "Epoch 27/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0058\n",
            "Epoch 28/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0055\n",
            "Epoch 29/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0055\n",
            "Epoch 30/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0055\n",
            "Epoch 31/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0054\n",
            "Epoch 32/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0054\n",
            "Epoch 33/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0052\n",
            "Epoch 34/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0051\n",
            "Epoch 35/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0052\n",
            "Epoch 36/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0051\n",
            "Epoch 37/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0051\n",
            "Epoch 38/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0051\n",
            "Epoch 39/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0049\n",
            "Epoch 40/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0050\n",
            "Epoch 41/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0049\n",
            "Epoch 42/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0050\n",
            "Epoch 43/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0048\n",
            "Epoch 44/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0049\n",
            "Epoch 45/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0049\n",
            "Epoch 46/100\n",
            "89/89 - 1s - 12ms/step - loss: 0.0049\n",
            "Epoch 47/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0048\n",
            "Epoch 48/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0047\n",
            "Epoch 49/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0047\n",
            "Epoch 50/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0047\n",
            "Epoch 51/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0046\n",
            "Epoch 52/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0046\n",
            "Epoch 53/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0047\n",
            "Epoch 54/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0047\n",
            "Epoch 55/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0047\n",
            "Epoch 56/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0046\n",
            "Epoch 57/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0046\n",
            "Epoch 58/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0045\n",
            "Epoch 59/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0045\n",
            "Epoch 60/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0046\n",
            "Epoch 61/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0045\n",
            "Epoch 62/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0045\n",
            "Epoch 63/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0044\n",
            "Epoch 64/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0045\n",
            "Epoch 65/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0045\n",
            "Epoch 66/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0045\n",
            "Epoch 67/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0044\n",
            "Epoch 68/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0045\n",
            "Epoch 69/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0045\n",
            "Epoch 70/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0045\n",
            "Epoch 71/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0045\n",
            "Epoch 72/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0043\n",
            "Epoch 73/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0044\n",
            "Epoch 74/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0043\n",
            "Epoch 75/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0046\n",
            "Epoch 76/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0045\n",
            "Epoch 77/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0044\n",
            "Epoch 78/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0044\n",
            "Epoch 79/100\n",
            "89/89 - 1s - 12ms/step - loss: 0.0045\n",
            "Epoch 80/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0045\n",
            "Epoch 81/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0044\n",
            "Epoch 82/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0044\n",
            "Epoch 83/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0043\n",
            "Epoch 84/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0044\n",
            "Epoch 85/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0044\n",
            "Epoch 86/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0043\n",
            "Epoch 87/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0044\n",
            "Epoch 88/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0043\n",
            "Epoch 89/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0042\n",
            "Epoch 90/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0044\n",
            "Epoch 91/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0043\n",
            "Epoch 92/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0042\n",
            "Epoch 93/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0044\n",
            "Epoch 94/100\n",
            "89/89 - 1s - 12ms/step - loss: 0.0043\n",
            "Epoch 95/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0042\n",
            "Epoch 96/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0042\n",
            "Epoch 97/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0042\n",
            "Epoch 98/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0044\n",
            "Epoch 99/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0043\n",
            "Epoch 100/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0042\n",
            "WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000024949BDF100> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 292ms/stepWARNING:tensorflow:6 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000024949BDF100> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step\n",
            "   • KINTA, PERAK: RMSE=0.186, MAE=0.103\n",
            "Epoch 1/100\n",
            "89/89 - 7s - 73ms/step - loss: 0.7045\n",
            "Epoch 2/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.1461\n",
            "Epoch 3/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0716\n",
            "Epoch 4/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0480\n",
            "Epoch 5/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0330\n",
            "Epoch 6/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0252\n",
            "Epoch 7/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0194\n",
            "Epoch 8/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0155\n",
            "Epoch 9/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0131\n",
            "Epoch 10/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0111\n",
            "Epoch 11/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0097\n",
            "Epoch 12/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0086\n",
            "Epoch 13/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0076\n",
            "Epoch 14/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0075\n",
            "Epoch 15/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0064\n",
            "Epoch 16/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0060\n",
            "Epoch 17/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0058\n",
            "Epoch 18/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0059\n",
            "Epoch 19/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0053\n",
            "Epoch 20/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0051\n",
            "Epoch 21/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0048\n",
            "Epoch 22/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0045\n",
            "Epoch 23/100\n",
            "89/89 - 1s - 12ms/step - loss: 0.0045\n",
            "Epoch 24/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0044\n",
            "Epoch 25/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0043\n",
            "Epoch 26/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0041\n",
            "Epoch 27/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0041\n",
            "Epoch 28/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0041\n",
            "Epoch 29/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0039\n",
            "Epoch 30/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0039\n",
            "Epoch 31/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0037\n",
            "Epoch 32/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0037\n",
            "Epoch 33/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0036\n",
            "Epoch 34/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0037\n",
            "Epoch 35/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0036\n",
            "Epoch 36/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0036\n",
            "Epoch 37/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0036\n",
            "Epoch 38/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0034\n",
            "Epoch 39/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0035\n",
            "Epoch 40/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0034\n",
            "Epoch 41/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0034\n",
            "Epoch 42/100\n",
            "89/89 - 1s - 11ms/step - loss: 0.0034\n",
            "Epoch 43/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0033\n",
            "Epoch 44/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0034\n",
            "Epoch 45/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0033\n",
            "Epoch 46/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0033\n",
            "Epoch 47/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0032\n",
            "Epoch 48/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0032\n",
            "Epoch 49/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0032\n",
            "Epoch 50/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0032\n",
            "Epoch 51/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0033\n",
            "Epoch 52/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0033\n",
            "Epoch 53/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0032\n",
            "Epoch 54/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0031\n",
            "Epoch 55/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0033\n",
            "Epoch 56/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0031\n",
            "Epoch 57/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0032\n",
            "Epoch 58/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0030\n",
            "Epoch 59/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0031\n",
            "Epoch 60/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0031\n",
            "Epoch 61/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0030\n",
            "Epoch 62/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0030\n",
            "Epoch 63/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0031\n",
            "Epoch 64/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0032\n",
            "Epoch 65/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0033\n",
            "Epoch 66/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0032\n",
            "Epoch 67/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0030\n",
            "Epoch 68/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0029\n",
            "Epoch 69/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0030\n",
            "Epoch 70/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0030\n",
            "Epoch 71/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0029\n",
            "Epoch 72/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0030\n",
            "Epoch 73/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0030\n",
            "Epoch 74/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0030\n",
            "Epoch 75/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0029\n",
            "Epoch 76/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0029\n",
            "Epoch 77/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0029\n",
            "Epoch 78/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0029\n",
            "Epoch 79/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0029\n",
            "Epoch 80/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0029\n",
            "Epoch 81/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0029\n",
            "Epoch 82/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0029\n",
            "Epoch 83/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0029\n",
            "Epoch 84/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0029\n",
            "Epoch 85/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0029\n",
            "Epoch 86/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0029\n",
            "Epoch 87/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0028\n",
            "Epoch 88/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0028\n",
            "Epoch 89/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0028\n",
            "Epoch 90/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0028\n",
            "Epoch 91/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0028\n",
            "Epoch 92/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0028\n",
            "Epoch 93/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0028\n",
            "Epoch 94/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0028\n",
            "Epoch 95/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0028\n",
            "Epoch 96/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0027\n",
            "Epoch 97/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0027\n",
            "Epoch 98/100\n",
            "89/89 - 1s - 8ms/step - loss: 0.0027\n",
            "Epoch 99/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0027\n",
            "Epoch 100/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0027\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step\n",
            "   • KLANG, SELANGOR: RMSE=0.179, MAE=0.090\n",
            "Epoch 1/100\n",
            "89/89 - 7s - 74ms/step - loss: 0.6062\n",
            "Epoch 2/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.1351\n",
            "Epoch 3/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0708\n",
            "Epoch 4/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0478\n",
            "Epoch 5/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0359\n",
            "Epoch 6/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0287\n",
            "Epoch 7/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0237\n",
            "Epoch 8/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0201\n",
            "Epoch 9/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0175\n",
            "Epoch 10/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0148\n",
            "Epoch 11/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0138\n",
            "Epoch 12/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0122\n",
            "Epoch 13/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0111\n",
            "Epoch 14/100\n",
            "89/89 - 1s - 15ms/step - loss: 0.0102\n",
            "Epoch 15/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0095\n",
            "Epoch 16/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0087\n",
            "Epoch 17/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0082\n",
            "Epoch 18/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0078\n",
            "Epoch 19/100\n",
            "89/89 - 1s - 9ms/step - loss: 0.0076\n",
            "Epoch 20/100\n",
            "89/89 - 1s - 10ms/step - loss: 0.0070\n",
            "Epoch 21/100\n"
          ]
        }
      ],
      "source": [
        "# ─── Imports & Seeds ───────────────────────────────────────────────────────────\n",
        "import os, glob, random, time\n",
        "import numpy as np, pandas as pd\n",
        "import tensorflow as tf\n",
        "from math import sqrt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense, Dropout, LayerNormalization,\n",
        "    MultiHeadAttention, Conv1D, Add\n",
        ")\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# ─── PARAMETERS ────────────────────────────────────────────────────────────────\n",
        "INPUT_DIR   = \"data/raw\"\n",
        "OUTPUT_DIR  = \"outputs\"\n",
        "\n",
        "# ensure directories exist\n",
        "os.makedirs(INPUT_DIR,  exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# window & split sizes\n",
        "WINDOW_SIZE = 7\n",
        "SEQ_LENGTH  = WINDOW_SIZE - 1\n",
        "\n",
        "# train‑set size per crop\n",
        "TRAIN_SIZES = {\n",
        "    \"Cucumber\": 2826,\n",
        "    \"LongBean\": 2826,\n",
        "    \"Tomato\":   2826\n",
        "}\n",
        "\n",
        "# model hyperparameters\n",
        "D_MODEL       = 64\n",
        "N_HEADS       = 4\n",
        "CONV_FILTERS  = 64\n",
        "CONV_KERNEL   = 4\n",
        "DROPOUT_RATE  = 0.2\n",
        "N_LAYERS      = 2\n",
        "\n",
        "# training\n",
        "EPOCHS     = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "# ─── MODEL DEFINITION ──────────────────────────────────────────────────────────\n",
        "def transformer_model_with_cnn(\n",
        "    seq_len, num_feats, d_model, n_heads,\n",
        "    conv_filters, conv_kernel, dropout_rate, n_layers\n",
        "):\n",
        "    inp = Input(shape=(seq_len, num_feats))\n",
        "    x = inp\n",
        "\n",
        "    # attention + residual\n",
        "    for _ in range(n_layers):\n",
        "        attn = MultiHeadAttention(num_heads=n_heads, key_dim=d_model)(x, x)\n",
        "        attn = Dropout(dropout_rate)(attn)\n",
        "        attn = LayerNormalization(epsilon=1e-6)(attn)\n",
        "        x = Add()([x, attn])\n",
        "\n",
        "    # convolution + residual\n",
        "    for _ in range(n_layers):\n",
        "        c = Conv1D(filters=conv_filters, kernel_size=conv_kernel,\n",
        "                   activation=\"relu\", padding=\"same\")(x)\n",
        "        c = Dropout(dropout_rate)(c)\n",
        "        c = LayerNormalization(epsilon=1e-6)(c)\n",
        "        x = Add()([x, c])\n",
        "\n",
        "    out = Dense(units=num_feats)(x)\n",
        "    return Model(inputs=inp, outputs=out)\n",
        "\n",
        "\n",
        "# ─── SHEET PROCESSOR ───────────────────────────────────────────────────────────\n",
        "def process_sheet(df, train_size):\n",
        "    # parse & index by date\n",
        "    df[\"Tarikh\"] = pd.to_datetime(df[\"Tarikh\"]).dt.strftime(\"%d/%m/%Y\")\n",
        "    df.index = df[\"Tarikh\"]\n",
        "    df = df.drop(\"Tarikh\", axis=1).apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "    # target series\n",
        "    series = df[\"WholesalePriceNew\"].values.reshape(-1, 1)\n",
        "    train, test = series[:train_size], series[train_size:]\n",
        "\n",
        "    # scale 0–1\n",
        "    scaler = MinMaxScaler((0, 1))\n",
        "    train_s = scaler.fit_transform(train)\n",
        "    test_s  = scaler.transform(test)\n",
        "\n",
        "    # sliding windows\n",
        "    def slide(arr, w): return np.array([arr[i:i+w] for i in range(len(arr)-w+1)])\n",
        "    w = WINDOW_SIZE\n",
        "    tx = slide(train_s, w); ty = tx[:, -1]; tx = tx[:, :-1]\n",
        "    vx = slide(test_s, w);  vy = vx[:, -1]; vx = vx[:, :-1]\n",
        "\n",
        "    # build & train\n",
        "    model = transformer_model_with_cnn(\n",
        "        seq_len=w-1,\n",
        "        num_feats=1,\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        conv_filters=CONV_FILTERS,\n",
        "        conv_kernel=CONV_KERNEL,\n",
        "        dropout_rate=DROPOUT_RATE,\n",
        "        n_layers=N_LAYERS\n",
        "    )\n",
        "    model.compile(loss=MeanSquaredError(), optimizer=Adam(1e-3))\n",
        "    model.fit(tx, ty, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2)\n",
        "\n",
        "    # predict & invert scale\n",
        "    preds = model.predict(vx)[:, -1, :]\n",
        "    preds_r = scaler.inverse_transform(preds)\n",
        "    vy_r    = scaler.inverse_transform(vy)\n",
        "\n",
        "    rmse = sqrt(mean_squared_error(vy_r, preds_r))\n",
        "    mae  = mean_absolute_error(vy_r, preds_r)\n",
        "    return vy_r, preds_r, rmse, mae\n",
        "\n",
        "\n",
        "# ─── MAIN LOOP ─────────────────────────────────────────────────────────────────\n",
        "for fp in glob.glob(os.path.join(INPUT_DIR, \"*_FillKNN.xlsx\")):\n",
        "    crop = os.path.basename(fp).split(\"_\")[0]\n",
        "    ts   = TRAIN_SIZES.get(crop)\n",
        "    out_file = f\"{crop}_STCE_results.xlsx\"\n",
        "\n",
        "    # load & prepare writer\n",
        "    xls    = pd.ExcelFile(fp)\n",
        "    writer = pd.ExcelWriter(os.path.join(OUTPUT_DIR, out_file),\n",
        "                            engine=\"xlsxwriter\")\n",
        "\n",
        "    print(f\"\\n➡️ Processing {crop}…\")\n",
        "    for sheet in xls.sheet_names:\n",
        "        df = pd.read_excel(fp, sheet_name=sheet)\n",
        "        actual, pred, rmse, mae = process_sheet(df, ts)\n",
        "\n",
        "        # save sheet\n",
        "        out_df = pd.DataFrame({\n",
        "            \"Actual\":    actual.flatten(),\n",
        "            \"Predicted\": pred.flatten(),\n",
        "            \"RMSE\":      rmse,\n",
        "            \"MAE\":       mae\n",
        "        })\n",
        "        out_df.to_excel(writer, sheet_name=sheet, index=False)\n",
        "        print(f\"   • {sheet}: RMSE={rmse:.3f}, MAE={mae:.3f}\")\n",
        "\n",
        "    writer.close()\n",
        "    print(f\"✅ Saved: {os.path.join(OUTPUT_DIR, out_file)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. TCE: Transformer + CNN + Positional Encoding + MLP\n",
        "Positional Encoding: sinusoidal\n",
        "\n",
        "Architecture: Dense→PosEnc→Dropout → n×(Attention + residual) → n×(Conv1D + residual) → MLP(128→64) → Dense output\n",
        "\n",
        "Metrics: RMSE, MAE, MAPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1_kfFZmc---c"
      },
      "outputs": [],
      "source": [
        "# ─── Imports & Seeds ───────────────────────────────────────────────────────────\n",
        "import os, glob, random, time\n",
        "import numpy as np, pandas as pd\n",
        "import tensorflow as tf\n",
        "from math import sqrt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import (Dense, Dropout, LayerNormalization,\n",
        "                                     MultiHeadAttention, Conv1D, Add, Layer)\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# ─── PARAMETERS ────────────────────────────────────────────────────────────────\n",
        "INPUT_DIR   = \"data/raw\"\n",
        "OUTPUT_DIR  = \"outputs\"\n",
        "\n",
        "# ensure directories exist\n",
        "os.makedirs(INPUT_DIR,  exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# train set size per crop (adjust as needed)\n",
        "TRAIN_SIZES = {\n",
        "    \"Cucumber\": 2826,\n",
        "    \"LongBean\": 2826,\n",
        "    \"Tomato\":   2826,\n",
        "    \"KubisBulat\": 2856\n",
        "}\n",
        "\n",
        "# sliding‑window\n",
        "WINDOW_SIZE = 7\n",
        "SEQ_LENGTH  = WINDOW_SIZE - 1\n",
        "\n",
        "# model hyperparameters\n",
        "D_MODEL       = 64\n",
        "N_HEADS       = 4\n",
        "CONV_FILTERS  = 64\n",
        "CONV_KERNEL   = 12\n",
        "DROPOUT_RATE  = 0.2\n",
        "N_LAYERS      = 2\n",
        "MLP_UNITS_1   = 128\n",
        "MLP_UNITS_2   = 64\n",
        "\n",
        "# training params\n",
        "EPOCHS     = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "# ─── Positional Encoding Layer ────────────────────────────────────────────────\n",
        "class PositionalEncoding(Layer):\n",
        "    def __init__(self, d_model, **kwargs):\n",
        "        super(PositionalEncoding, self).__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def get_config(self):\n",
        "        cfg = super(PositionalEncoding, self).get_config()\n",
        "        cfg.update({\"d_model\": self.d_model})\n",
        "        return cfg\n",
        "\n",
        "    def call(self, x):\n",
        "        pos = tf.range(tf.shape(x)[1], dtype=tf.float32)[:, tf.newaxis]\n",
        "        div = tf.exp(tf.range(0, self.d_model, 2, dtype=tf.float32) *\n",
        "                     (-tf.math.log(10000.0) / self.d_model))\n",
        "        angle = pos * div\n",
        "        pe = tf.concat([tf.sin(angle), tf.cos(angle)], axis=-1)[tf.newaxis, ...]\n",
        "        return x + pe\n",
        "\n",
        "\n",
        "# ─── Model Definition ──────────────────────────────────────────────────────────\n",
        "def build_tce_model(seq_len, num_feats):\n",
        "    inp = Input(shape=(seq_len, num_feats))\n",
        "    # project to d_model dims\n",
        "    x = Dense(D_MODEL)(inp)\n",
        "    # add positional encoding\n",
        "    x = PositionalEncoding(D_MODEL)(x)\n",
        "    x = Dropout(DROPOUT_RATE)(x)\n",
        "\n",
        "    # self‑attention blocks\n",
        "    for _ in range(N_LAYERS):\n",
        "        attn = MultiHeadAttention(num_heads=N_HEADS, key_dim=D_MODEL)(x, x)\n",
        "        attn = Dropout(DROPOUT_RATE)(attn)\n",
        "        attn = LayerNormalization(epsilon=1e-6)(attn)\n",
        "        x = Add()([x, attn])\n",
        "\n",
        "    # convolutional residual blocks\n",
        "    for _ in range(N_LAYERS):\n",
        "        c = Conv1D(filters=CONV_FILTERS, kernel_size=CONV_KERNEL,\n",
        "                   activation=\"relu\", padding=\"same\")(x)\n",
        "        c = Dropout(DROPOUT_RATE)(c)\n",
        "        c = LayerNormalization(epsilon=1e-6)(c)\n",
        "        # ensure dims match\n",
        "        if c.shape[-1] != x.shape[-1]:\n",
        "            x = Dense(c.shape[-1])(x)\n",
        "        x = Add()([x, c])\n",
        "\n",
        "    # MLP head\n",
        "    x = Dense(MLP_UNITS_1, activation=\"relu\")(x)\n",
        "    x = Dropout(DROPOUT_RATE)(x)\n",
        "    x = Dense(MLP_UNITS_2, activation=\"relu\")(x)\n",
        "    x = Dropout(DROPOUT_RATE)(x)\n",
        "\n",
        "    out = Dense(num_feats)(x)\n",
        "    return Model(inputs=inp, outputs=out)\n",
        "\n",
        "\n",
        "# ─── Sheet Processor ───────────────────────────────────────────────────────────\n",
        "def process_sheet(df, train_size):\n",
        "    # format and index\n",
        "    df[\"Tarikh\"] = pd.to_datetime(df[\"Tarikh\"]).dt.strftime(\"%d/%m/%Y\")\n",
        "    df.index = df[\"Tarikh\"]\n",
        "    df = df.drop(\"Tarikh\", axis=1).apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "    series = df[\"WholesalePriceNew\"].values.reshape(-1, 1)\n",
        "    train, test = series[:train_size], series[train_size:]\n",
        "\n",
        "    # scale 0–1\n",
        "    scaler = MinMaxScaler((0, 1))\n",
        "    train_s = scaler.fit_transform(train)\n",
        "    test_s  = scaler.transform(test)\n",
        "\n",
        "    # sliding-window\n",
        "    def slide(arr, w): return np.array([arr[i:i+w] for i in range(len(arr)-w+1)])\n",
        "    w = WINDOW_SIZE\n",
        "    tx = slide(train_s, w); ty = tx[:, -1]; tx = tx[:, :-1]\n",
        "    vx = slide(test_s, w);  vy = vx[:, -1]; vx = vx[:, :-1]\n",
        "\n",
        "    # build & train\n",
        "    model = build_tce_model(seq_len=w-1, num_feats=1)\n",
        "    model.compile(loss=MeanSquaredError(), optimizer=Adam(1e-3))\n",
        "    model.fit(tx, ty, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2)\n",
        "\n",
        "    # predict & invert scaling\n",
        "    preds = model.predict(vx)[:, -1, :]\n",
        "    preds_r = scaler.inverse_transform(preds)\n",
        "    vy_r    = scaler.inverse_transform(vy)\n",
        "\n",
        "    # metrics\n",
        "    rmse = sqrt(mean_squared_error(vy_r, preds_r))\n",
        "    mae  = mean_absolute_error(vy_r, preds_r)\n",
        "    mape = mean_absolute_percentage_error(vy_r, preds_r) * 100\n",
        "    return vy_r, preds_r, rmse, mae, mape\n",
        "\n",
        "\n",
        "# ─── Main Loop ─────────────────────────────────────────────────────────────────\n",
        "for fp in glob.glob(os.path.join(INPUT_DIR, \"*_FillKNN.xlsx\")):\n",
        "    crop      = os.path.basename(fp).split(\"_\")[0]\n",
        "    train_sz  = TRAIN_SIZES.get(crop, WINDOW_SIZE*10)  # fallback if missing\n",
        "    out_fname = f\"{crop}_TCE_results.xlsx\"\n",
        "\n",
        "    xls    = pd.ExcelFile(fp)\n",
        "    writer = pd.ExcelWriter(os.path.join(OUTPUT_DIR, out_fname),\n",
        "                            engine=\"xlsxwriter\")\n",
        "\n",
        "    print(f\"\\n➡️ Running TCE on {crop}…\")\n",
        "    for sheet in xls.sheet_names:\n",
        "        df = pd.read_excel(fp, sheet_name=sheet)\n",
        "        actual, pred, rmse, mae, mape = process_sheet(df, train_sz)\n",
        "\n",
        "        results = pd.DataFrame({\n",
        "            \"Actual\":    actual.flatten(),\n",
        "            \"Predicted\": pred.flatten(),\n",
        "            \"RMSE\":      rmse,\n",
        "            \"MAE\":       mae,\n",
        "            \"MAPE\":      mape\n",
        "        })\n",
        "        results.to_excel(writer, sheet_name=sheet, index=False)\n",
        "        print(f\"   • {sheet}: RMSE={rmse:.3f}, MAE={mae:.3f}, MAPE={mape:.2f}%\")\n",
        "\n",
        "    writer.close()\n",
        "    print(f\"✅ Saved → {os.path.join(OUTPUT_DIR, out_fname)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. STLE: Transformer + LSTM\n",
        "Architecture: n×(Multi‑Head Attention + residual) → n×(LSTM + residual) → Dense output\n",
        "\n",
        "Metrics: RMSE, MAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT59DwnV32Wj"
      },
      "outputs": [],
      "source": [
        "# ─── Imports & Seeds ───────────────────────────────────────────────────────────\n",
        "import os, glob, random, time\n",
        "import numpy as np, pandas as pd\n",
        "import tensorflow as tf\n",
        "from math import sqrt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense, Dropout, LayerNormalization,\n",
        "    MultiHeadAttention, LSTM, Add\n",
        ")\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# ─── PARAMETERS ────────────────────────────────────────────────────────────────\n",
        "INPUT_DIR   = \"data/raw\"\n",
        "OUTPUT_DIR  = \"outputs\"\n",
        "\n",
        "# ensure directories exist\n",
        "os.makedirs(INPUT_DIR,  exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# train‑set size per crop\n",
        "TRAIN_SIZES = {\n",
        "    \"Cucumber\":   2826,\n",
        "    \"LongBean\":   2826,\n",
        "    \"KubisBulat\": 2856,\n",
        "    \"Tomato\":     2886\n",
        "}\n",
        "\n",
        "# sliding‑window\n",
        "WINDOW_SIZE = 7\n",
        "SEQ_LENGTH  = WINDOW_SIZE - 1\n",
        "\n",
        "# model hyperparameters\n",
        "D_MODEL      = 64\n",
        "N_HEADS      = 4\n",
        "LSTM_UNITS   = 64\n",
        "DROPOUT_RATE = 0.2\n",
        "N_LAYERS     = 2\n",
        "\n",
        "# training\n",
        "EPOCHS     = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "# ─── MODEL DEFINITION ──────────────────────────────────────────────────────────\n",
        "def transformer_model_with_lstm(seq_len, num_feats, d_model, n_heads, lstm_units, dropout_rate, n_layers):\n",
        "    inp = Input(shape=(seq_len, num_feats))\n",
        "    x   = inp\n",
        "\n",
        "    # self‑attention + residual\n",
        "    for _ in range(n_layers):\n",
        "        attn = MultiHeadAttention(num_heads=n_heads, key_dim=d_model)(x, x)\n",
        "        attn = Dropout(dropout_rate)(attn)\n",
        "        attn = LayerNormalization(epsilon=1e-6)(attn)\n",
        "        x    = Add()([x, attn])\n",
        "\n",
        "    # LSTM + residual\n",
        "    for _ in range(n_layers):\n",
        "        lstm = LSTM(units=lstm_units, return_sequences=True)(x)\n",
        "        lstm = Dropout(dropout_rate)(lstm)\n",
        "        lstm = LayerNormalization(epsilon=1e-6)(lstm)\n",
        "        x    = Add()([x, lstm])\n",
        "\n",
        "    out = Dense(units=num_feats)(x)\n",
        "    return Model(inputs=inp, outputs=out)\n",
        "\n",
        "\n",
        "# ─── SHEET PROCESSOR ───────────────────────────────────────────────────────────\n",
        "def process_sheet(df, train_size):\n",
        "    # format & index\n",
        "    df[\"Tarikh\"] = pd.to_datetime(df[\"Tarikh\"]).dt.strftime(\"%d/%m/%Y\")\n",
        "    df.index     = df[\"Tarikh\"]\n",
        "    df = df.drop(\"Tarikh\", axis=1).apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "    # target series\n",
        "    series = df[\"WholesalePriceNew\"].values.reshape(-1, 1)\n",
        "    train, test = series[:train_size], series[train_size:]\n",
        "\n",
        "    # scale to [0,1]\n",
        "    scaler = MinMaxScaler((0, 1))\n",
        "    train_s = scaler.fit_transform(train)\n",
        "    test_s  = scaler.transform(test)\n",
        "\n",
        "    # sliding windows\n",
        "    def slide(arr, w):\n",
        "        return np.array([arr[i : i + w] for i in range(len(arr) - w + 1)])\n",
        "    w  = WINDOW_SIZE\n",
        "    tx = slide(train_s, w); ty = tx[:, -1]; tx = tx[:, :-1]\n",
        "    vx = slide(test_s,  w); vy = vx[:, -1]; vx = vx[:, :-1]\n",
        "\n",
        "    # build & train\n",
        "    model = transformer_model_with_lstm(\n",
        "        seq_len      = w - 1,\n",
        "        num_feats    = 1,\n",
        "        d_model      = D_MODEL,\n",
        "        n_heads      = N_HEADS,\n",
        "        lstm_units   = LSTM_UNITS,\n",
        "        dropout_rate = DROPOUT_RATE,\n",
        "        n_layers     = N_LAYERS\n",
        "    )\n",
        "    model.compile(loss=MeanSquaredError(), optimizer=Adam(1e-3))\n",
        "    model.fit(tx, ty, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2)\n",
        "\n",
        "    # predict & invert scale\n",
        "    preds   = model.predict(vx)[:, -1, :]\n",
        "    preds_r = scaler.inverse_transform(preds)\n",
        "    vy_r    = scaler.inverse_transform(vy)\n",
        "\n",
        "    # compute metrics\n",
        "    rmse = sqrt(mean_squared_error(vy_r, preds_r))\n",
        "    mae  = mean_absolute_error(vy_r, preds_r)\n",
        "    return vy_r, preds_r, rmse, mae\n",
        "\n",
        "\n",
        "# ─── MAIN LOOP ─────────────────────────────────────────────────────────────────\n",
        "for fp in glob.glob(os.path.join(INPUT_DIR, \"*_FillKNN.xlsx\")):\n",
        "    crop     = os.path.basename(fp).split(\"_\")[0]\n",
        "    ts       = TRAIN_SIZES.get(crop, WINDOW_SIZE*10)\n",
        "    out_file = f\"{crop}_STLE_results.xlsx\"\n",
        "\n",
        "    xls    = pd.ExcelFile(fp)\n",
        "    writer = pd.ExcelWriter(os.path.join(OUTPUT_DIR, out_file), engine=\"xlsxwriter\")\n",
        "\n",
        "    print(f\"\\n➡️ Processing {crop} with STLE…\")\n",
        "    for sheet in xls.sheet_names:\n",
        "        df     = pd.read_excel(fp, sheet_name=sheet)\n",
        "        actual, pred, rmse, mae = process_sheet(df, ts)\n",
        "\n",
        "        results = pd.DataFrame({\n",
        "            \"Actual\":    actual.flatten(),\n",
        "            \"Predicted\": pred.flatten(),\n",
        "            \"RMSE\":      rmse,\n",
        "            \"MAE\":       mae\n",
        "        })\n",
        "        results.to_excel(writer, sheet_name=sheet, index=False)\n",
        "        print(f\"   • {sheet}: RMSE={rmse:.3f}, MAE={mae:.3f}\")\n",
        "\n",
        "    writer.close()\n",
        "    print(f\"✅ Saved → {os.path.join(OUTPUT_DIR, out_file)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. TFE: Transformer + FFN\n",
        "Flow: n×(Attention + residual) → n×(Position‑wise Feed‑Forward + residual) → Dense\n",
        "\n",
        "Metrics: RMSE, MAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVO6EJh3jkF6"
      },
      "outputs": [],
      "source": [
        "# ─── Imports & Seeds ───────────────────────────────────────────────────────────\n",
        "import os, glob, random, time\n",
        "import numpy as np, pandas as pd\n",
        "import tensorflow as tf\n",
        "from math import sqrt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, MultiHeadAttention, Add\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# ─── PARAMETERS ────────────────────────────────────────────────────────────────\n",
        "INPUT_DIR   = \"data/raw\"\n",
        "OUTPUT_DIR  = \"outputs\"\n",
        "\n",
        "# ensure directories exist\n",
        "os.makedirs(INPUT_DIR,  exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# train‑set size per crop\n",
        "TRAIN_SIZES = {\n",
        "    \"Cucumber\":   2826,\n",
        "    \"LongBean\":   2826,\n",
        "    \"KubisBulat\": 2856,\n",
        "    \"Tomato\":     2886\n",
        "}\n",
        "\n",
        "# sliding‑window\n",
        "WINDOW_SIZE = 7\n",
        "SEQ_LENGTH  = WINDOW_SIZE - 1\n",
        "\n",
        "# model hyperparameters\n",
        "D_MODEL      = 64\n",
        "N_HEADS      = 4\n",
        "DENSE_UNITS  = 128\n",
        "DROPOUT_RATE = 0.2\n",
        "N_LAYERS     = 2\n",
        "\n",
        "# training\n",
        "EPOCHS     = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "# ─── MODEL DEFINITION ──────────────────────────────────────────────────────────\n",
        "def build_tfe_model(seq_len, num_feats):\n",
        "    inp = Input(shape=(seq_len, num_feats))\n",
        "    x   = inp\n",
        "\n",
        "    # Multi‑Head Self‑Attention + residual\n",
        "    for _ in range(N_LAYERS):\n",
        "        attn = MultiHeadAttention(num_heads=N_HEADS, key_dim=D_MODEL)(x, x)\n",
        "        attn = Dropout(DROPOUT_RATE)(attn)\n",
        "        attn = LayerNormalization(epsilon=1e-6)(attn)\n",
        "        x    = Add()([x, attn])\n",
        "\n",
        "    # Position‑wise Feed‑Forward + residual\n",
        "    for _ in range(N_LAYERS):\n",
        "        ff = Dense(units=DENSE_UNITS, activation=\"relu\")(x)\n",
        "        ff = Dropout(DROPOUT_RATE)(ff)\n",
        "        ff = LayerNormalization(epsilon=1e-6)(ff)\n",
        "        x  = Add()([x, ff])\n",
        "\n",
        "    out = Dense(units=num_feats)(x)\n",
        "    return Model(inputs=inp, outputs=out)\n",
        "\n",
        "\n",
        "# ─── SHEET PROCESSOR ───────────────────────────────────────────────────────────\n",
        "def process_sheet(df, train_size):\n",
        "    # format dates & index\n",
        "    df[\"Tarikh\"] = pd.to_datetime(df[\"Tarikh\"]).dt.strftime(\"%d/%m/%Y\")\n",
        "    df.index     = df[\"Tarikh\"]\n",
        "    df = df.drop(\"Tarikh\", axis=1).apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "    # extract series\n",
        "    series = df[\"WholesalePriceNew\"].values.reshape(-1, 1)\n",
        "    train, test = series[:train_size], series[train_size:]\n",
        "\n",
        "    # scale to [0,1]\n",
        "    scaler = MinMaxScaler((0, 1))\n",
        "    train_s = scaler.fit_transform(train)\n",
        "    test_s  = scaler.transform(test)\n",
        "\n",
        "    # sliding windows\n",
        "    def slide(arr, w): return np.array([arr[i:i+w] for i in range(len(arr)-w+1)])\n",
        "    w  = WINDOW_SIZE\n",
        "    tx = slide(train_s, w); ty = tx[:, -1]; tx = tx[:, :-1]\n",
        "    vx = slide(test_s,  w); vy = vx[:, -1]; vx = vx[:, :-1]\n",
        "\n",
        "    # build & train\n",
        "    model = build_tfe_model(seq_len=w-1, num_feats=1)\n",
        "    model.compile(loss=MeanSquaredError(), optimizer=Adam(1e-3))\n",
        "    model.fit(tx, ty, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2)\n",
        "\n",
        "    # predict & invert scaling\n",
        "    preds   = model.predict(vx)[:, -1, :]\n",
        "    preds_r = scaler.inverse_transform(preds)\n",
        "    vy_r    = scaler.inverse_transform(vy)\n",
        "\n",
        "    # metrics\n",
        "    rmse = sqrt(mean_squared_error(vy_r, preds_r))\n",
        "    mae  = mean_absolute_error(vy_r, preds_r)\n",
        "    return vy_r, preds_r, rmse, mae\n",
        "\n",
        "\n",
        "# ─── MAIN LOOP ─────────────────────────────────────────────────────────────────\n",
        "for filepath in glob.glob(os.path.join(INPUT_DIR, \"*_FillKNN.xlsx\")):\n",
        "    crop      = os.path.basename(filepath).split(\"_\")[0]\n",
        "    train_sz  = TRAIN_SIZES.get(crop, WINDOW_SIZE*10)  # fallback\n",
        "    out_fname = f\"{crop}_TFE_results.xlsx\"\n",
        "\n",
        "    xls    = pd.ExcelFile(filepath)\n",
        "    writer = pd.ExcelWriter(os.path.join(OUTPUT_DIR, out_fname),\n",
        "                            engine=\"xlsxwriter\")\n",
        "\n",
        "    print(f\"\\n➡️ Processing {crop} with TFE…\")\n",
        "    for sheet in xls.sheet_names:\n",
        "        df      = pd.read_excel(filepath, sheet_name=sheet)\n",
        "        actual, pred, rmse, mae = process_sheet(df, train_sz)\n",
        "\n",
        "        results = pd.DataFrame({\n",
        "            \"Actual\":    actual.flatten(),\n",
        "            \"Predicted\": pred.flatten(),\n",
        "            \"RMSE\":      rmse,\n",
        "            \"MAE\":       mae\n",
        "        })\n",
        "        results.to_excel(writer, sheet_name=sheet, index=False)\n",
        "        print(f\"   • {sheet}: RMSE={rmse:.3f}, MAE={mae:.3f}\")\n",
        "\n",
        "    writer.close()\n",
        "    print(f\"✅ Saved → {os.path.join(OUTPUT_DIR, out_fname)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. LSTM use as the Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dip6J4YZjctD"
      },
      "outputs": [],
      "source": [
        "# ─── Imports & Seeds ───────────────────────────────────────────────────────────\n",
        "import os, glob, random, time\n",
        "import numpy as np, pandas as pd\n",
        "import tensorflow as tf\n",
        "from math import sqrt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# ─── PARAMETERS ────────────────────────────────────────────────────────────────\n",
        "INPUT_DIR    = \"data/raw\"\n",
        "OUTPUT_DIR   = \"outputs\"\n",
        "TIMESTEPS    = 7     # sliding window length\n",
        "DAY_PREDICT  = 90    # how many days at end used for testing\n",
        "EPOCHS       = 100\n",
        "BATCH_SIZE   = 32\n",
        "\n",
        "# ensure directories exist\n",
        "os.makedirs(INPUT_DIR,  exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# ─── SHEET PROCESSOR ───────────────────────────────────────────────────────────\n",
        "def process_sheet(df):\n",
        "    # format dates & index\n",
        "    df[\"Tarikh\"] = pd.to_datetime(df[\"Tarikh\"]).dt.strftime(\"%d/%m/%Y\")\n",
        "    df.index     = df[\"Tarikh\"]\n",
        "    df = df.drop(\"Tarikh\", axis=1).apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "    # extract series and reshape\n",
        "    series = df[\"WholesalePriceNew\"].values.reshape(-1, 1)\n",
        "\n",
        "    # train/test split\n",
        "    train_len = len(series) - DAY_PREDICT\n",
        "    train, test = series[:train_len], series[train_len:]\n",
        "\n",
        "    # scale to [0,1]\n",
        "    scaler = MinMaxScaler((0, 1))\n",
        "    train_s = scaler.fit_transform(train)\n",
        "    test_s  = scaler.transform(test)\n",
        "\n",
        "    # create sliding windows\n",
        "    x_train, y_train = [], []\n",
        "    for i in range(TIMESTEPS, len(train_s)):\n",
        "        x_train.append(train_s[i-TIMESTEPS:i, 0])\n",
        "        y_train.append(train_s[i, 0])\n",
        "    x_train = np.array(x_train).reshape(-1, TIMESTEPS, 1)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    # build LSTM model\n",
        "    model = Sequential([\n",
        "        LSTM(100, return_sequences=True, input_shape=(TIMESTEPS, 1)),\n",
        "        LSTM(100, return_sequences=False),\n",
        "        Dense(50, activation=\"relu\"),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(1e-3), loss=\"mse\")\n",
        "    model.fit(x_train, y_train, epochs=EPOCHS,\n",
        "              batch_size=BATCH_SIZE, verbose=2)\n",
        "\n",
        "    # prepare test windows\n",
        "    x_test = []\n",
        "    full = np.vstack((train_s[-TIMESTEPS:], test_s))\n",
        "    for i in range(TIMESTEPS, len(full)):\n",
        "        x_test.append(full[i-TIMESTEPS:i, 0])\n",
        "    x_test = np.array(x_test).reshape(-1, TIMESTEPS, 1)\n",
        "\n",
        "    # predict & invert scale\n",
        "    preds = model.predict(x_test)\n",
        "    preds_r = scaler.inverse_transform(preds)\n",
        "    y_test  = test.reshape(-1, 1)\n",
        "\n",
        "    # metrics\n",
        "    rmse = sqrt(mean_squared_error(y_test, preds_r))\n",
        "    mae  = mean_absolute_error(y_test, preds_r)\n",
        "    mape = mean_absolute_percentage_error(y_test, preds_r) * 100\n",
        "\n",
        "    return y_test.flatten(), preds_r.flatten(), rmse, mae, mape\n",
        "\n",
        "\n",
        "# ─── MAIN LOOP ─────────────────────────────────────────────────────────────────\n",
        "for fp in glob.glob(os.path.join(INPUT_DIR, \"*_FillKNN.xlsx\")):\n",
        "    crop      = os.path.basename(fp).split(\"_\")[0]\n",
        "    out_fname = f\"{crop}_LSTM_results.xlsx\"\n",
        "    writer    = pd.ExcelWriter(os.path.join(OUTPUT_DIR, out_fname),\n",
        "                               engine=\"xlsxwriter\")\n",
        "\n",
        "    print(f\"\\n➡️ Running LSTM for {crop}…\")\n",
        "    xls = pd.ExcelFile(fp)\n",
        "    for sheet in xls.sheet_names:\n",
        "        df = pd.read_excel(fp, sheet_name=sheet)\n",
        "        actual, preds, rmse, mae, mape = process_sheet(df)\n",
        "\n",
        "        # assemble results\n",
        "        out_df = pd.DataFrame({\n",
        "            \"Actual\":    actual,\n",
        "            \"Predicted\": preds,\n",
        "            \"RMSE\":      rmse,\n",
        "            \"MAE\":       mae,\n",
        "            \"MAPE\":      mape\n",
        "        })\n",
        "        out_df.to_excel(writer, sheet_name=sheet, index=False)\n",
        "        print(f\"   • {sheet}: RMSE={rmse:.3f}, MAE={mae:.3f}, MAPE={mape:.2f}%\")\n",
        "\n",
        "    writer.close()\n",
        "    print(f\"✅ Saved → {os.path.join(OUTPUT_DIR, out_fname)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-MH5wbIjMiD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'pip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Conv1D, Flatten\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
          ]
        }
      ],
      "source": [
        "# ─── Imports & Seeds ───────────────────────────────────────────────────────────\n",
        "import os, glob, random, time\n",
        "import numpy as np, pandas as pd\n",
        "import tensorflow as tf\n",
        "from math import sqrt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# ─── PARAMETERS ────────────────────────────────────────────────────────────────\n",
        "INPUT_DIR   = \"data/raw\"\n",
        "OUTPUT_DIR  = \"outputs\"\n",
        "\n",
        "TIMESTEPS   = 7    # sliding window length\n",
        "DAY_PREDICT = 90   # days at end used as test set\n",
        "EPOCHS      = 100\n",
        "BATCH_SIZE  = 32\n",
        "\n",
        "# ensure directories exist\n",
        "os.makedirs(INPUT_DIR,  exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# ─── SHEET PROCESSOR ───────────────────────────────────────────────────────────\n",
        "def process_sheet(df):\n",
        "    # format dates & index\n",
        "    df[\"Tarikh\"] = pd.to_datetime(df[\"Tarikh\"]).dt.strftime(\"%d/%m/%Y\")\n",
        "    df.index     = df[\"Tarikh\"]\n",
        "    df = df.drop(\"Tarikh\", axis=1).apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "    # extract series\n",
        "    series = df[\"WholesalePriceNew\"].values.reshape(-1, 1)\n",
        "\n",
        "    # train/test split\n",
        "    train_len = len(series) - DAY_PREDICT\n",
        "    train, test = series[:train_len], series[train_len:]\n",
        "\n",
        "    # scale to [0,1]\n",
        "    scaler = MinMaxScaler((0, 1))\n",
        "    train_s = scaler.fit_transform(train)\n",
        "    test_s  = scaler.transform(test)\n",
        "\n",
        "    # build training windows\n",
        "    x_train, y_train = [], []\n",
        "    for i in range(TIMESTEPS, len(train_s)):\n",
        "        x_train.append(train_s[i-TIMESTEPS:i, 0])\n",
        "        y_train.append(train_s[i, 0])\n",
        "    x_train = np.array(x_train).reshape(-1, TIMESTEPS, 1)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    # build model\n",
        "    model = Sequential([\n",
        "        Conv1D(64, 3, activation=\"relu\", input_shape=(TIMESTEPS, 1)),\n",
        "        Flatten(),\n",
        "        Dense(50, activation=\"relu\"),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(1e-3), loss=\"mse\")\n",
        "    model.fit(x_train, y_train, epochs=EPOCHS,\n",
        "              batch_size=BATCH_SIZE, verbose=2)\n",
        "\n",
        "    # build test windows (include last train window)\n",
        "    full = np.vstack((train_s[-TIMESTEPS:], test_s))\n",
        "    x_test = [ full[i-TIMESTEPS:i, 0] for i in range(TIMESTEPS, len(full)) ]\n",
        "    x_test = np.array(x_test).reshape(-1, TIMESTEPS, 1)\n",
        "\n",
        "    # predict & unscale\n",
        "    preds    = model.predict(x_test)\n",
        "    preds_r  = scaler.inverse_transform(preds)\n",
        "    y_test_r = test.reshape(-1, 1)\n",
        "\n",
        "    # metrics\n",
        "    rmse = sqrt(mean_squared_error(y_test_r, preds_r))\n",
        "    mae  = mean_absolute_error(y_test_r, preds_r)\n",
        "    mape = mean_absolute_percentage_error(y_test_r, preds_r) * 100\n",
        "\n",
        "    return y_test_r.flatten(), preds_r.flatten(), rmse, mae, mape\n",
        "\n",
        "\n",
        "# ─── MAIN LOOP ─────────────────────────────────────────────────────────────────\n",
        "for fp in glob.glob(os.path.join(INPUT_DIR, \"*_FillKNN.xlsx\")):\n",
        "    crop      = os.path.basename(fp).split(\"_\")[0]\n",
        "    out_fname = f\"{crop}_CNN_results.xlsx\"\n",
        "    writer    = pd.ExcelWriter(os.path.join(OUTPUT_DIR, out_fname),\n",
        "                               engine=\"xlsxwriter\")\n",
        "\n",
        "    print(f\"\\n➡️ Running CNN for {crop}…\")\n",
        "    xls = pd.ExcelFile(fp)\n",
        "    for sheet in xls.sheet_names:\n",
        "        df = pd.read_excel(fp, sheet_name=sheet)\n",
        "        actual, preds, rmse, mae, mape = process_sheet(df)\n",
        "\n",
        "        # assemble and save\n",
        "        out_df = pd.DataFrame({\n",
        "            \"Actual\":    actual,\n",
        "            \"Predicted\": preds,\n",
        "            \"RMSE\":      rmse,\n",
        "            \"MAE\":       mae,\n",
        "            \"MAPE\":      mape\n",
        "        })\n",
        "        out_df.to_excel(writer, sheet_name=sheet, index=False)\n",
        "        print(f\"   • {sheet}: RMSE={rmse:.3f}, MAE={mae:.3f}, MAPE={mape:.2f}%\")\n",
        "\n",
        "    writer.close()\n",
        "    print(f\"✅ Saved → {os.path.join(OUTPUT_DIR, out_fname)}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
